{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import cv2\n","import numpy as np\n","from deepface import DeepFace\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","###Used to detect emotion in order to test the stressing\n","def capture_frame():\n","    js = Javascript('''\n","    async function captureFrame() {\n","        const div = document.createElement('div');\n","        const video = document.createElement('video');\n","        const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n","\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        const ctx = canvas.getContext('2d');\n","        ctx.drawImage(video, 0, 0);\n","\n","        stream.getTracks().forEach(track => track.stop());\n","        video.remove();\n","\n","        const data = canvas.toDataURL('image/jpeg');\n","        return data;\n","    }\n","    captureFrame();\n","    ''')\n","    display(js)\n","    data = eval_js('captureFrame()')\n","    data = data.split(',')[1]\n","    return b64decode(data)\n","\n","def analyze_emotion(img):\n","    try:\n","        analysis = DeepFace.analyze(img, actions=['emotion'], enforce_detection=False)\n","        if isinstance(analysis, list):\n","            analysis = analysis[0]\n","        return analysis['dominant_emotion'], analysis['emotion']\n","    except Exception as e:\n","        print(f\"Error in emotion detection: {str(e)}\")\n","        return None, None\n","\n","def draw_results(img, face_detections, emotion, emotions_dict):\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    if face_detections and emotion:\n","        for face in face_detections:\n","            facial_area = face['facial_area']\n","            x = facial_area['x']\n","            y = facial_area['y']\n","            w = facial_area['w']\n","            h = facial_area['h']\n","\n","            cv2.rectangle(img_rgb, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","\n","\n","            fig, ax = plt.subplots(figsize=(8, 2))\n","            emotions = list(emotions_dict.keys())\n","            values = list(emotions_dict.values())\n","\n","            ax.barh(emotions, values, color='skyblue')\n","            ax.set_xlabel('Confidence (%)')\n","            ax.set_title('Emotion Analysis')\n","\n","\n","            fig.canvas.draw()\n","            plot_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n","            plot_img = plot_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n","            plt.close()\n","\n","            label = f\"{emotion.upper()}: {emotions_dict[emotion]:.1f}%\"\n","            cv2.putText(img_rgb, label, (x, y - 10),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","\n","    return img_rgb\n","\n","try:\n","    while True:\n","        print(\"Press 'Stop' to interrupt.\")\n","        frame_data = capture_frame()\n","\n","        with open('current_frame.jpg', 'wb') as f:\n","            f.write(frame_data)\n","\n","        img = cv2.imread('current_frame.jpg')\n","\n","        face_detections = DeepFace.extract_faces(img, enforce_detection=False)\n","        dominant_emotion, emotions_dict = analyze_emotion(img)\n","\n","        if dominant_emotion and emotions_dict:\n","\n","            result_image = draw_results(img, face_detections, dominant_emotion, emotions_dict)\n","            plt.figure(figsize=(12, 8))\n","            plt.imshow(result_image)\n","            plt.axis('off')\n","            plt.show()\n","        else:\n","            print(\"No face detected or error in emotion analysis\")\n","\n","        plt.close('all')\n","\n","except KeyboardInterrupt:\n","    print(\"Video feed stopped.\")"],"cell_type":"code","metadata":{"id":"a2P522uhTM3n","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import cv2\n","import numpy as np\n","from deepface import DeepFace\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import warnings\n","warnings.filterwarnings('ignore')\n","#stress detection\n","def capture_frame():\n","    js = Javascript('''\n","    async function captureFrame() {\n","        const div = document.createElement('div');\n","        const video = document.createElement('video');\n","        const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n","\n","        document.body.appendChild(div);\n","        div.appendChild(video);\n","        video.srcObject = stream;\n","        await video.play();\n","\n","        const canvas = document.createElement('canvas');\n","        canvas.width = video.videoWidth;\n","        canvas.height = video.videoHeight;\n","        const ctx = canvas.getContext('2d');\n","        ctx.drawImage(video, 0, 0);\n","\n","        stream.getTracks().forEach(track => track.stop());\n","        video.remove();\n","\n","        const data = canvas.toDataURL('image/jpeg');\n","        return data;\n","    }\n","    captureFrame();\n","    ''')\n","    display(js)\n","    data = eval_js('captureFrame()')\n","    data = data.split(',')[1]\n","    return b64decode(data)\n","\n","def analyze_emotion(img):\n","    try:\n","        analysis = DeepFace.analyze(img, actions=['emotion'], enforce_detection=False)\n","        if isinstance(analysis, list):\n","            analysis = analysis[0]\n","        return analysis['dominant_emotion'], analysis['emotion']\n","    except Exception as e:\n","        print(f\"Error in emotion detection: {str(e)}\")\n","        return None, None\n","\n","def analyze_stress(emotions_dict):\n","    sorted_emotions = sorted(emotions_dict.items(), key=lambda x: x[1], reverse=True)\n","    top_3_emotions = sorted_emotions[:3]\n","\n","    emotion_names = [e[0] for e in top_3_emotions]\n","    emotion_scores = [e[1] for e in top_3_emotions]\n","\n","    stress_level = 0\n","    stress_indicators = []\n","\n","\n","    if 'angry' in emotion_names[:2] and 'fear' in emotion_names[:2]:\n","        if emotion_names[2] in ['neutral', 'disgust']:\n","            stress_level = 3  # High stress if angry and fear are in top 2\n","            stress_indicators.append(\"Primary stress pattern detected\")\n","\n","    elif ('angry' in emotion_names[:2] or 'fear' in emotion_names[:2]):\n","        if 'disgust' in emotion_names[:2]:\n","            stress_level = 2  # Moderate stress if angry / fear are in top two paired with disgust as the other emotion\n","            stress_indicators.append(\"Secondary stress pattern detected\")\n","\n","    total_negative = sum(emotions_dict[emotion] for emotion in ['angry', 'fear', 'disgust'])\n","    if total_negative > 70:\n","        stress_level = max(stress_level, 2)\n","        stress_indicators.append(\"High negative emotion intensity\")\n","\n","    if emotions_dict['happy'] < 10:\n","        stress_level = max(stress_level, 1)\n","        stress_indicators.append(\"Low happiness detected\")\n","\n","    return {\n","        'stress_level': stress_level,\n","        'indicators': stress_indicators,\n","        'top_emotions': top_3_emotions\n","    }\n","\n","def draw_results(img, face_detections, stress_analysis, emotions_dict):\n","    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","    if face_detections:\n","        for face in face_detections:\n","            facial_area = face['facial_area']\n","            x = facial_area['x']\n","            y = facial_area['y']\n","            w = facial_area['w']\n","            h = facial_area['h']\n","\n","            stress_colors = {\n","                0: (0, 255, 0),    # Green for no stress\n","                1: (255, 255, 0),  # Yellow for mild stress\n","                2: (255, 165, 0),  # Orange for moderate stress\n","                3: (255, 0, 0)     # Red for high stress\n","            }\n","\n","            color = stress_colors[stress_analysis['stress_level']]\n","            cv2.rectangle(img_rgb, (x, y), (x + w, y + h), color, 2)\n","            stress_levels = ['No', 'Mild', 'Moderate', 'High']\n","            stress_text = f\"Stress Level: {stress_levels[stress_analysis['stress_level']]}\"\n","            y_offset = 30\n","            cv2.putText(img_rgb, stress_text, (x, y - y_offset),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n","\n","            fig, ax = plt.subplots(figsize=(8, 3))\n","            emotions = list(emotions_dict.keys())\n","            values = list(emotions_dict.values())\n","\n","            ax.barh(emotions, values, color='skyblue')\n","            ax.set_xlabel('Confidence (%)')\n","            ax.set_title('Emotion Analysis')\n","            plt.close()\n","\n","    return img_rgb\n","\n","try:\n","    while True:\n","        print(\"Press 'Stop' to interrupt.\")\n","        frame_data = capture_frame()\n","        with open('current_frame.jpg', 'wb') as f:\n","            f.write(frame_data)\n","\n","        img = cv2.imread('current_frame.jpg')\n","\n","        face_detections = DeepFace.extract_faces(img, enforce_detection=False)\n","        dominant_emotion, emotions_dict = analyze_emotion(img)\n","\n","        if dominant_emotion and emotions_dict:\n","            stress_analysis = analyze_stress(emotions_dict)\n","            result_image = draw_results(img, face_detections, stress_analysis, emotions_dict)\n","            if stress_analysis['indicators']:\n","                print(\"\\nStress Indicators Detected:\")\n","                for indicator in stress_analysis['indicators']:\n","                    print(f\"- {indicator}\")\n","                print(\"\\nTop 3 Emotions:\")\n","                for emotion, score in stress_analysis['top_emotions']:\n","                    print(f\"- {emotion.capitalize()}: {score:.1f}%\")\n","\n","            plt.figure(figsize=(12, 8))\n","            plt.imshow(result_image)\n","            plt.axis('off')\n","            plt.show()\n","        else:\n","            print(\"No face detected or error in emotion analysis\")\n","\n","        plt.close('all')\n","\n","except KeyboardInterrupt:\n","    print(\"Video feed stopped.\")"],"metadata":{"id":"ilaykxZKDRFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from keras import models\n","from google.colab.patches import cv2_imshow\n","\n","###OLD!!!!\n","emotion_model = models.load_model('face_model.h5')\n","emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'] #labeluri pt emotii\n","\n","\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","\n","img = cv2.imread('selectie2.jpg')\n","if img is None:\n","    print(\"Imaginea nu a fost încarcata corect!\")\n","    exit()\n","\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","cv2_imshow(gray)  # imagine in grey filter\n","print(\"Shape:\", gray.shape)\n","\n","\n","faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n","print(\"Fete detectate:\", faces)\n","\n","if len(faces) == 0:\n","    print(\"Nu au fost detectate fete!\")\n","\n","for (x, y, w, h) in faces:\n","    # rois\n","    face = gray[y:y + h, x:x + w]\n","    face = cv2.resize(face, (48, 48))  # redimensiune\n","    face = face / 255.0\n","    face = np.reshape(face, (1, 48, 48, 1))\n","\n","    # Prezice emoția\n","    emotion_prediction = emotion_model.predict(face)\n","    emotion_index = np.argmax(emotion_prediction)\n","    emotion_text = emotion_labels[emotion_index]\n","\n","    # dreptunghi\n","    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Dreptunghi rosu\n","    cv2.putText(img, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n","\n","cv2_imshow(img)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n"],"metadata":{"id":"hVIDhvliTxkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RakpCWGEHrpA"},"execution_count":null,"outputs":[]}]}